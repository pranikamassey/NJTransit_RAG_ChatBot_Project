# src/ingest.py


##steps for referencee
#source .venv/bin/activate

## Ingest the first PDF, overwriting old index/meta:
# python src/ingest.py \
#   --pdf data/AccessLinkCustomerGuidelines.pdf \
#   --index-output data/index.faiss \
#   --meta-output data/meta.json

## Ingest the second PDF, but this time *append* to the existing index/meta:
# python src/ingest.py \
#   --pdf data/AnotherPolicyDoc.pdf \
#   --index-output data/index.faiss \
#   --meta-output data/meta.json \
#   --append



import argparse
import json
import os

import faiss
import numpy as np
import pdfplumber
import openai

from src.common import OPENAI_API_KEY

def chunk_text(text, max_tokens=400, overlap=50):
    words = text.split()
    chunks, start = [], 0
    while start < len(words):
        end = min(start + max_tokens, len(words))
        chunks.append(" ".join(words[start:end]))
        start += max_tokens - overlap
    return chunks

def main(pdf_path, index_output, meta_output):
    print("ðŸŸ¢ Starting ingestionâ€¦")

    # 1) Read & chunk
    all_chunks, meta = [], []
    with pdfplumber.open(pdf_path) as pdf:
        for page_no, page in enumerate(pdf.pages, start=1):
            text = page.extract_text() or ""
            for chunk in chunk_text(text):
                all_chunks.append(chunk)
                meta.append({"page": page_no, "text": chunk})
    print(f"ðŸ”¸ Extracted {len(all_chunks)} chunks.")

    # 2) Embed via new API
    openai.api_key = OPENAI_API_KEY
    embeddings = []
    for i, chunk in enumerate(all_chunks, start=1):
        
        resp = openai.embeddings.create(
            model="text-embedding-ada-002",
            input=chunk
        )
    # pull out the embedding from the response object
        embedding = resp.data[0].embedding
        embeddings.append(embedding)

        if i % 50 == 0:
            print(f"ðŸ”¹ Embedded {i}/{len(all_chunks)}")
    print("ðŸ”µ Embeddings complete.")

    # 3) Build FAISS index
    dim   = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings, dtype="float32"))
    print("ðŸ”¶ FAISS index built.")

    # 4) Save index & metadata
    os.makedirs(os.path.dirname(index_output), exist_ok=True)
    faiss.write_index(index, index_output)
    with open(meta_output, "w") as f:
        json.dump(meta, f, indent=2)
    print(f"ðŸŸ£ Wrote index to {index_output} and meta to {meta_output}")

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--pdf",          required=True)
    p.add_argument("--index-output", default="data/index.faiss")
    p.add_argument("--meta-output",  default="data/meta.json")
    args = p.parse_args()
    main(args.pdf, args.index_output, args.meta_output)
